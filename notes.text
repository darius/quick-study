[parts of these notes are out of date]

First plan:

 X first fumblings
 - FSM language
 - PEG or pushdown-automata language
 - Forth
 - CPS Scheme. (Maybe skip here straight from FSM?)
 - Something actually useful (in assembler or Lua?)

------------------------------------------------------------------
Tracing jit = explanation-based generalization

EBG starts with a general definition and a concrete problem to solve
with it. It deploys general methods to solve it, recording its
'operational' actions. Then it saves the record, generalized, to use
on future problems, hopefully more efficiently. This is simple in
principle, but to do this efficiently you need to make good decisions
about which examples to record and how to generalize them.

A tracing compiler starts from a program and an execution state at a
particular control-flow node. It runs forward some number of steps,
recording its primitive actions. Then it saves the record, generalized
and compiled, to use on future visits to the same node. Once again you
want to make economical decisions about which traces to record and
what aspects of them to generalize from the trace (e.g. the value of a
variable often varies when its type doesn't).

(Similar, but speculative: a data structure optimizer starts from an
operation on a particular object. It records what's done, and in
particular what's accessed through multiple links from the starting
object. The record is used to alter the layout of similar objects for
more direct access -- perhaps only of newly-allocated ones. Both data
and code need to be altered, in coordination. Yet again we need smarts
about what to do this to.)

It's sort of a commonplace that a tracing jit does specialization, or
partial evaluation. I think EBG is an even closer fit. (It's true that
EBG=PE, in that the core algorithm in Prolog is the same for each; but
the same *relation* doesn't mean the same *function* -- see below.)

If you look at the code in the EBG=PE paper, basically all you need to
do to get a tracing compiler is define the 'operational' predicate to
be true on recursive calls to the goal.

The PyPy folks have a tracing jit for Prolog, but they don't seem to
have remarked on the connection to EBG.

(So how about an example-guided partial evaluator? It could do
opportunistic reductions like an online PE, but still produce
fully-general (but guarded) code like an offline PE with all-dynamic
arguments. This corresponds to the guards and side exits of a tracing
jit compiler. If we trace *multiple* examples before generating code,
that's like a trace-tree based compiler; though of course one run of
the program can produce multiple examples of calls to individual
functions.)

The previous paragraph amounts to a source-level tracing jit, right?
A "tracing ahead-of-time compiler".

EBG extracts the conditions on the arguments that allow more-general
calls to make the same decisions. An example of this in a tracing jit
is specializing on the argument type.

A note about "EBG = PE": isn't that weird, the name for this in one
community is generalization, while in the other community it's
specialization! But no, it makes sense: EBG and PE deal in the same
*relation*: a special trace from a general definition. What you call
the relation depends on which end you start from. (TODO: characterize
exactly how EBG and PE call on the EBG/PE algorithm with
differently-variabilized arguments. Also how the core algorithm fails
on realistic use cases of the different kinds.)

Can we somehow show the idea of specializing data representations
in a simplified setting too?

http://en.wikipedia.org/wiki/Explanation-based_learning

------------------------------------------------------------------
The tracing-jit-compiler papers I've read have described quite a few
heuristics. Can we simplify? What if we could economically measure the
effect of speculative decisions -- then it'd 'just' be a matter of the
system trying out compiling actions and using what works.

(That 'just' could hide a lot: explore vs. exploit, estimating which
differences are significant, tracking changes to the program's
behavior over time (when decisions that used to improve things no
longer do), and computing and using 'exchange rates': marginal prices
to rationally trade off resources like time and memory. Also, it
doesn't excuse us from predicting, because one of the major expenses
is recording a trace, which may be used only once, so finding out that
you shouldn't have recorded that trace doesn't directly help you avoid
more bad decisions -- it's not like trying out a fully compiled trace
tree and deciding you'd rather go back to the older version.)

More basically there's the mechanics of how to compare the runs for a
larger trace-tree vs. the *relevant* runs of a smaller trace-tree and
its side exit.

If the compiler compiles itself, there's a need for metareasoning
about how much effort that deserves. Does that just fall out?

So, can we make a rational compiler?

This idea of just trying stuff out is kind of similar to Dybvig's
paper on his Scheme inliner (which doesn't use runtime data, though).

------------------------------------------------------------------
It seems standard AFAIK to abandon a trace when it gets too long. This
makes sense for two reasons: a finite trace-buffer and because longer
traces are less likely to get repeated. However, that unlikelihood
goes up with the number of control-flow decisions, not the number of
instructions. This suggests a different criterion to abandon a trace.

------------------------------------------------------------------
Would it help to use some kind of loop detector (like
tortois-and-hare) on the traces instead of assuming the loop starts
where we guessed it?

What if we never abandon recording a trace, we just record into a
circular buffer? We'd need to be able to recover the starting point
when we detect a loop. Kind of nice for simplicity: we don't need
alternative tracing and nontracing actions in the interpreter.

------------------------------------------------------------------
How well does the equality-saturation approach to optimization mesh
with trace trees? Does it have advantages over something like the SPUR
idea of relying on an SMT solver? (Is it even really different?)

------------------------------------------------------------------
[really obsolete] OK, a plan for explaining this in a blog post:

Start with lis.py trimmed down to purely-functional. Augment into a
PE/EBG as in the PE=EBG paper. Add a header argument and make calls to
it be 'operational'. Talk about all the stuff this explanation glosses
over.

Also worth mentioning in post: the win in tracing is not really from
avoiding compiling cold code; it's that trace trees are much
easier/quicker to optimize than general flowgraphs.

------------------------------------------------------------------
In PyPy, they changed the goal and operationality criterion to look at
the data as well as the control flow, in order to trace loops of the
user-level program instead of the interpreter. Let's, uh, generalize
this as explanation-based generalization, or try to.

Rather like multistage partial evaluation. What would "multistage EBL"
look like?

------------------------------------------------------------------
Wild idea: make our operationality criterion be *data parallel ops*.
Send jitted code off to the GPU or something. Crazy!

------------------------------------------------------------------
Multistage EBL shouldn't necessary (pointed out by Johnicholas)

Classic tracing jits compile out one level of interpretation
overhead. For a tower of interpreters they're not fundamentally better
than a classical compiler, AFAIK. That's why PyPy has a special API to
tell the tracing jit that it's tracing an interpreter, and what the
program counter of that interpreter is, etc. Seems ad-hoc.

LZ compression is said to be universal. What this means precisely, I
don't quite know; but in this note I'll take it as pointing the way to
a tracing jit that adapts to higher-order patterns.

IIRC http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.5816
introduces superoperators (chunks of bytecodes) as something like LZ
compression applied to the sequences of byte-ops that occur at
runtime, but with that data collected from a preliminary profiling run
instead of at 'real' runtime. Also, they apply it to a fairly small
number of the most profitable sequences from the profiling, where LZ
compression would get started right away, like an aggressive JIT
compiler. (I don't remember if they draw this analogy to LZ
compression... probably. They surely did say you can focus your
superoperator choice towards saving either of space or time, by
looking at static or dynamic instruction counts.)

A more recent paper that I remember better and that may be a closer
fit:
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.7676

I implemented pretty much the same scheme in opcodes.awk at
http://idel.cvs.sourceforge.net/viewvc/idel/idel/src/
(It takes stats from profiling runs and generates new opcodes with C
code chunking the opcode sequences, with literal operands specialized
in; plus peephole-optimizer tables for the compiler to use to emit
them.)

So, most basically we could start interpreting bytecodes, watch them
go by, and just run an LZ compressor on the bytecode stream, patching
the program as we invent new operators (no longer fitting in a
byte).

The first problem you run into: if you chunked together operators that
straddle a basic-block boundary, that'd break the program. (The papers
explain this.) I guess it's simplest to make the compressor respect
this constraint, but see below.

Next problem: memory for the history. Simplest solution: when you run
out of space, restart the compressor from nothing. But really
forgetting all you'd learned would require undoing all the patches to
the program, too; this seems painful, so I'm going to resign myself
for now to an ever-growing set of chunks. Also, instead of clearing
the compression buffer periodically, we could make it a bounded
circular buffer.

The next concern: isn't this too expensive? I'd worry about that
later: presumably we can complicate it with some kind of dilution of
the overhead, profiling first for hotspots.

This kind of chunk isn't as good for global optimization as the loop
traces of the tracing jits I've read about. After all, many loops
aren't a single basic block. Tracing jits get away with chunking
across them because the same original code can appear multiple times
in multiple forms after the compiler's messed with it: it's
"polyvariant". So consider other answers to the first problem. If you
decide it's profitable to create a 'forbidden' chunk, you'll need to
duplicate parts of the program according to where the control flows
from -- so that the chunking's allowed in the places you care
about. The cost going into this decision has to include the code-size
expansion. Maybe this'd be nicer to think about in terms of
interpreting Scheme instead of bytecodes?

At least one other thing the tracing jits have going for them: paying
attention to the dynamic data processed in their traces (typically
just the type of the data, I think). Not going to worry about it
tonight, important as it is.

Back to the goal that triggered this: how well would this work on
towers of interpreters?

------------------------------------------------------------------
Probably simpler than the above scheme: superoperators a la Proebsting
or Ertl, but dropping the constraint of preserving basic-block
boundaries, as above.

------------------------------------------------------------------
Refs:

http://news.ycombinator.com/item?id=1645820
http://github.com/resistor/BrainFTracing2
http://github.com/resistor/BrainFTracing
http://lambda-the-ultimate.org/node/3851
http://www.ics.uci.edu/~franz/
http://morepypy.blogspot.com/2010/07/comparing-spur-to-pypy.html
http://lambda-the-ultimate.org/node/3806
http://andreasgal.wordpress.com/2008/06/
http://github.com/aemoncannon/rubinius
http://slidefinder.net/t/trace_fragment_selection_method_based/12114382
